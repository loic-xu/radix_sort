---
title: 'Projet 6 : Radix sort'
author: "Adèle BERGER, Hajar LAMTAAI, Loïc XU"
output:
  pdf_document: default
  html_document: default
---


# Présentation du problème

L'objectif principal de ce projet est de comparer deux algorithmes de tri en fonction de leur efficacité : le Radix Sort et un algorithme de tri par comparaison classique tel que Merge Sort ou Quick Sort, qui ont une complexité de O(n log n). Le Radix Sort, contrairement aux algorithmes de tri traditionnels, trie les éléments en procédant digit par digit, offrant ainsi une approche non-comparative avec une complexité théorique de O(n) dans certains cas. L'enjeu du projet est de déterminer dans quels cas un algorithme est plus efficace que l'autre, en fonction de la taille des données, et de mettre en évidence la transition entre un algorithme plus rapide à petite échelle et un autre plus performant à plus grande échelle.

# Difficulté algorithmique

## Problème combinatoire

Le problème du tri est un problème combinatoire fondamental dans le domaine de l'algorithmique. Le but est d'organiser un ensemble de données dans un ordre donné, généralement croissant ou décroissant. Lorsque l'on utilise des algorithmes de tri par comparaison, chaque élément du tableau doit être comparé à d'autres éléments pour déterminer son emplacement relatif dans l'ordre. Ce processus de comparaison constitue un problème combinatoire, car le nombre de permutations possibles des éléments augmente rapidement avec la taille du tableau, créant ainsi une complexité de O(n log n) dans les meilleurs cas pour des algorithmes comme Quick Sort ou Merge Sort.


## Solution naïve

La solution naïve dans ce contexte consiste à utiliser des algorithmes de tri dont la complexité est O(n log n). Trois algorithmes populaires dans cette catégorie sont :

- Quick Sort : Un algorithme de tri par partitionnement qui fonctionne en divisant le tableau en sous-tableaux et en triant récursivement ces sous-tableaux. Sa complexité moyenne est O(n log n), bien qu’il puisse se dégrader à O(n²) dans le pire cas (lorsque le pivot choisi est défavorable).

- Merge Sort : Un algorithme de tri par fusion qui divise le tableau en deux parties, trie chacune d'elles, puis fusionne les sous-tableaux triés. Il est stable et garanti d'avoir une complexité de O(n log n) dans tous les cas, ce qui le rend plus fiable que Quick Sort.

- Heap Sort : Un algorithme de tri basé sur une structure de données appelée "tas" (heap). Il permet de trier les éléments en O(n log n) dans tous les cas, mais il n'est pas stable (il ne conserve pas l'ordre relatif des éléments égaux).

Ces trois algorithmes sont des choix naturels pour un tri efficace basé sur des comparaisons. Cependant, la question de savoir lequel est le plus efficace dépend de plusieurs facteurs, notamment la distribution des données et les caractéristiques spécifiques du problème à résoudre.



## Limites avec R et C++

```{r}
# Fonction simulant un tri et mesurant le temps d'exécution
one.simu_rcpp <- function(n, k, type, func) {
  v <- generate_values(n, k)  # Valeurs avec n éléments et k chiffres
  if (func == "quick_sort_Rcpp") t <- system.time(quick_sort_Rcpp(v))[[1]]
  if (func == "tri_base_Rcpp") t <- system.time(tri_base_Rcpp(v))[[1]]
  
  return(t)
}
# Définir le nombre de simulations et la taille des données
nbSimus <- 10  # Le nombre de simulations
vector_n <- seq(from = 100, to = 50100, length.out = nbSimus)


# Fusion Rcpp
res_fusion <- data.frame(matrix(0, nbSimus, 2))
colnames(res_fusion) <- c("n", "Time")
for (j in 1:nbSimus) {
  n_val <- vector_n[j]
  res_fusion[j,] <- c(n_val, one.simu(n_val, func = "tri_fusion_Rcpp"))
}

# Heap Rcpp
res_heap <- data.frame(matrix(0, nbSimus, 2))
colnames(res_heap) <- c("n", "Time")
for (j in 1:nbSimus) {
  n_val <- vector_n[j]
  res_heap[j,] <- c(n_val, one.simu(n_val, func = "heap_sort_Rcpp"))
}

# Quick Rcpp
res_quick_rcpp <- data.frame(matrix(0, nbSimus, 2))
colnames(res_quick_rcpp) <- c("n", "Time")
for (j in 1:nbSimus) {
  n_val <- vector_n[j]
  res_quick_rcpp[j,] <- c(n_val, one.simu(n_val, func = "quick_sort_Rcpp"))
}

# Init pour R
res_fusion_R <- data.frame(matrix(0, nbSimus, 2))
res_heap_R   <- data.frame(matrix(0, nbSimus, 2))
res_quick_R  <- data.frame(matrix(0, nbSimus, 2))
colnames(res_fusion_R) <- colnames(res_heap_R) <- colnames(res_quick_R) <- c("n", "Time")

# Boucle pour R
for (j in 1:nbSimus) {
  n_val <- vector_n[j]
  res_fusion_R[j,] <- c(n_val, one.simu(n_val, func = "tri_fusion"))
  res_heap_R[j,]   <- c(n_val, one.simu(n_val, func = "heap_sort"))
  res_quick_R[j,]  <- c(n_val, one.simu(n_val, func = "quick_sort"))
}

# Création du dataframe de comparaison avec les résultats
df_compar <- data.frame(
  n = vector_n,
  fusion_R = res_fusion_R$Time,
  fusion_Rcpp = res_fusion$Time,
  heap_R = res_heap_R$Time,
  heap_Rcpp = res_heap$Time,
  quick_R = res_quick_R$Time,
  quick_Rcpp = res_quick_rcpp$Time
)

# Transformation des résultats en format long pour ggplot
library(tidyr)
df_long <- pivot_longer(df_compar, cols = -n, names_to = "algo", values_to = "temps")


# Comparaison spécifique pour l'algorithme de tri fusion
df_fusion <- data.frame(
  n = vector_n,
  fusion_R = res_fusion_R$Time,
  fusion_Rcpp = res_fusion$Time
)
df_fusion_long <- pivot_longer(df_fusion, cols = -n, names_to = "algo", values_to = "temps")

ggplot(df_fusion_long, aes(x = n, y = temps, color = algo)) +
  geom_line(size = 1) +
  labs(title = "Comparaison R vs Rcpp - Tri Fusion",
       x = "Taille des données", y = "Temps (s)") +
  theme_minimal()

# Comparaison pour l'algorithme de heap
df_heap <- data.frame(
  n = vector_n,
  heap_R = res_heap_R$Time,
  heap_Rcpp = res_heap$Time
)
df_heap_long <- pivot_longer(df_heap, cols = -n, names_to = "algo", values_to = "temps")

ggplot(df_heap_long, aes(x = n, y = temps, color = algo)) +
  geom_line(size = 1) +
  labs(title = "Comparaison R vs Rcpp - Tri Heap",
       x = "Taille des données", y = "Temps (s)") +
  theme_minimal()

# Comparaison pour l'algorithme de tri rapide
df_quick <- data.frame(
  n = vector_n,
  quick_R = res_quick_R$Time,
  quick_Rcpp = res_quick_rcpp$Time
)
df_quick_long <- pivot_longer(df_quick, cols = -n, names_to = "algo", values_to = "temps")

ggplot(df_quick_long, aes(x = n, y = temps, color = algo)) +
  geom_line(size = 1) +
  labs(title = "Comparaison R vs Rcpp - Tri Rapide",
       x = "Taille des données", y = "Temps (s)") +
  theme_minimal()

# Calcul du ratio des temps R / Rcpp pour chaque algorithme
df_compar_ratio <- data.frame(
  n = vector_n,
  fusion_R_vs_Rcpp = res_fusion_R$Time / res_fusion$Time,
  heap_R_vs_Rcpp = res_heap_R$Time / res_heap$Time,
  quick_R_vs_Rcpp = res_quick_R$Time / res_quick_rcpp$Time
)

# Affichage des ratios
print("Comparaison des ratios Time_R / Time_Rcpp pour chaque algorithme:")
print(df_compar_ratio)

```



Les résultats des simulations ont permis de comparer les performances de plusieurs algorithmes de tri en R et en C++ via Rcpp pour différentes tailles de données. Les tests ont été effectués avec les algorithmes **Tri Fusion**, **Heap Sort**, et **Quick Sort**.

**Tri Fusion** en R et C++ a montré une différence de performance notable, avec des temps de calcul qui étaient infiniment longs pour des petites tailles de données (100 éléments) en R, mais beaucoup plus rapides en C++ via Rcpp. Pour des tailles plus grandes, comme 5655 éléments, la version C++ était environ 26 fois plus rapide que la version R en termes de temps de traitement. À des tailles de données encore plus grandes (par exemple, 50 100 éléments), la version C++ a continué à surpasser la version R, avec un facteur de gain proche de 14x.

**Heap Sort**, quant à lui, a également montré des améliorations de performance significatives en C++ comparé à R. À taille moyenne (par exemple, 5655 éléments), la version C++ était environ 27 fois plus rapide. Ce gain en efficacité était encore plus marqué pour des tailles plus importantes, comme 50 100 éléments, où la version C++ a été plus de 264 fois plus rapide que la version R.

Enfin, pour **Quick Sort**, les résultats étaient similaires, mais moins extrêmes. Bien que **Quick Sort** en C++ ait toujours montré un temps d'exécution plus court que sa version en R, le gain n'était pas aussi élevé que pour **Heap Sort**. À une taille de données de 5655 éléments, **Quick Sort** en C++ a été environ 41 fois plus rapide que la version en R. À des tailles encore plus grandes, comme 50 100 éléments, la version C++ a été environ 24 fois plus rapide que la version R.

En résumé, pour toutes les tailles de données testées, les versions en C++ via **Rcpp** se sont avérées beaucoup plus performantes que les versions en R, avec des gains de performance qui augmentent avec la taille des données. Les algorithmes en C++ ont permis de réduire significativement le temps de traitement, en particulier pour **Heap Sort**, qui a montré les plus grands gains de performance.



```{r}

# Comparaison des algorithmes Rcpp entre eux
df_rcpp_compar <- data.frame(
  n = vector_n,
  fusion_Rcpp = res_fusion$Time,
  heap_Rcpp = res_heap$Time,
  quick_Rcpp = res_quick_rcpp$Time
)

# Transformation des résultats en format long pour ggplot
df_rcpp_long <- pivot_longer(df_rcpp_compar, cols = -n, names_to = "algo", values_to = "temps")

# Graphique de la comparaison des algorithmes Rcpp
ggplot(df_rcpp_long, aes(x = n, y = temps, color = algo)) +
  geom_line(size = 1) +
  labs(title = "Comparaison des Algorithmes Rcpp",
       x = "Taille des données", y = "Temps (s)") +
  theme_minimal()



```

En comparant les performances des algorithmes en C++ via **Rcpp**, il apparaît que **Quick Sort** est l'algorithme le plus performant parmi ceux testés, avec des temps d'exécution nettement plus courts que ceux de **Tri Fusion** et **Heap Sort**, même pour des tailles de données importantes. Cette efficacité supérieure fait de **Quick Sort** le choix optimal pour la suite de notre étude. Ainsi, nous comparerons notre algorithme amélioré avec **Quick Sort** pour évaluer son efficacité en termes de performance et de complexité temporelle.



# Solution améliorée moderne

## Présentation de la stratégie algorithmique

Le **Radix Sort** est un algorithme de tri non comparatif qui fonctionne en triant les éléments chiffre par chiffre, du moins significatif au plus significatif (ou inversement, selon l'implémentation). Contrairement aux algorithmes de tri classiques comme le **quicksort**, le **mergesort**, ou le **heapsort**, qui comparent les éléments entre eux, le **Radix Sort** fonctionne par distribution des éléments en fonction de leurs chiffres individuels, ce qui le rend particulièrement efficace dans certains cas.

### Principe de fonctionnement de **Radix Sort** :

1. **Tri des éléments chiffre par chiffre** :  
   L'idée principale derrière **Radix Sort** est de trier les éléments en fonction de chaque chiffre individuel. Par exemple, pour des entiers, on commence par trier les éléments en fonction du chiffre des unités, puis par le chiffre des dizaines, des centaines, etc. Ce processus est itéré pour chaque position de chiffre jusqu'à ce que tous les chiffres aient été triés.

2. **Méthode de tri stable** :  
   **Radix Sort** utilise des algorithmes de tri stables à chaque étape. Cela signifie que, pour un même chiffre, les éléments qui ont des valeurs égales restent dans le même ordre qu'auparavant. Un exemple classique d'un tel algorithme de tri stable est le **Counting Sort**.

3. **Tri par base** :  
   Le terme "base" fait référence à la base du système numérique utilisé pour représenter les entiers (par exemple, base 10 pour les nombres décimaux, base 2 pour les nombres binaires). Le **Radix Sort** peut être utilisé avec différentes bases, mais la base 10 (décimale) est la plus courante pour les entiers.


### Complexité de **Radix Sort** :

La complexité du **Radix Sort** dépend principalement de deux facteurs :

- **n** : le nombre d'éléments à trier.
- **d** : le nombre de chiffres dans le plus grand élément.

La complexité de l'algorithme peut être décrite comme suit :

- Le **Radix Sort** nécessite de faire un tri stable en utilisant **Counting Sort** pour chaque chiffre.
- Le **Counting Sort** a une complexité de **O(n + k)**, où **n** est le nombre d'éléments et **k** est la taille de la plage des chiffres (par exemple, 10 pour les nombres décimaux, 256 pour les caractères ASCII, etc.).
- Puisque nous devons effectuer **d** itérations, où **d** est le nombre de chiffres du plus grand nombre (logarithmique par rapport à la valeur de l'élément maximum), la complexité globale du **Radix Sort** devient :

\[
O(d \times (n + k))
\]

Dans le meilleur des cas, si **k** est suffisamment petit (par exemple, pour les entiers représentés en base 10), cette complexité peut être réduite à **O(n)**, c'est-à-dire linéaire par rapport au nombre d'éléments.




## Simulations qui comparent les temps entre quick sort et radix sort 


Comparaison avec k = 3

```{r}
################################################
############# Setup & Données ##################
################################################

# Définir différentes tailles de n pour tester
vector_n <- seq(100, 50100, by = 5000)  # Tester de 100 à 50000 avec un pas de 5000
k <- 3  # Exemple de nombre de chiffres (k = 3 : valeurs entre 100 et 999)

# Types de données à tester
datasets <- c("random", "sorted", "reverse_sorted", "sorted_90")

# Fonction qui génère des nombres selon le nombre de chiffres (k)
generate_values <- function(n, k) {
  min_value <- 1  # Le minimum possible pour k chiffres (par exemple, 100 pour k=3)
  max_value <- 10^k - 1  # Le maximum possible pour k chiffres (par exemple, 999 pour k=3)
  return(sample(min_value:max_value, n, replace = TRUE))  # Génère des valeurs entre min_value et max_value
}

# Fonction qui mesure le temps d'exécution des algorithmes Rcpp de tri
one.simu_rcpp <- function(n, type = "random", func = "quick_sort_Rcpp") {
  
  # Génération des datasets selon l'intervalle défini par k
  if (type == "random") {
    v <- generate_values(n, k)  # Valeurs avec n éléments et k chiffres
  } else if (type == "sorted") {
    v <- sort(generate_values(n, k))  # Triées croissantes
  } else if (type == "reverse_sorted") {
    v <- sort(generate_values(n, k), decreasing = TRUE)  # Triées décroissantes
  } else if (type == "sorted_90") {
    # Générer 90% triées et 10% aléatoires
    sorted_part <- sort(generate_values(0.9 * n, k))  # 90% triées
    random_part <- generate_values(0.1 * n, k)  # 10% aléatoires
    v <- c(sorted_part, random_part)
  }
  
  # Appel des fonctions de tri
  if (func == "quick_sort_Rcpp") t <- system.time(quick_sort_Rcpp(v))[[1]]
  if (func == "tri_base_Rcpp") t <- system.time(tri_base_Rcpp(v))[[1]]
  
  return(t)
}

################################################################################################

###########################################################
############# Simulation à taille variable ##################
###########################################################

nbSimus <- length(vector_n)  # Le nombre de tailles de données testées

# Initialiser une liste pour stocker les résultats
results_list <- list()

# Simulations pour chaque type de dataset
for (dataset in datasets) {
  # Init des vecteurs pour stocker les temps
  time1 <- numeric(nbSimus)
  time2 <- numeric(nbSimus)
  
  # Simulations pour chaque taille de n
  for (i in 1:nbSimus) {
    # Calculer la médiane des temps pour chaque algorithme sur chaque taille de n
    time1[i] <- median(replicate(10, one.simu_rcpp(vector_n[i], type = dataset, func = "quick_sort_Rcpp")))
    time2[i] <- median(replicate(10, one.simu_rcpp(vector_n[i], type = dataset, func = "tri_base_Rcpp")))
  }
  
  # Stocker les résultats dans une liste
  results_list[[dataset]] <- data.frame(
    n = vector_n,
    quick_Rcpp = time1,
    base_Rcpp = time2
  )
}

# Packages
library(ggplot2)
library(tidyr)

# Transformer les résultats en format long (liste par dataset)
df_long_list <- lapply(names(results_list), function(dataset) {
  df <- results_list[[dataset]]
  df$dataset <- dataset
  pivot_longer(df, cols = -c(n, dataset), names_to = "algo", values_to = "temps")
})

# Afficher chaque graphique dans une fenêtre séparée
for (df_long in df_long_list) {
  dataset_name <- unique(df_long$dataset)
  
  # Plot
  print(
    ggplot(df_long, aes(x = n, y = temps, color = algo)) +
      geom_line(linewidth = 1) +
      labs(title = paste("Comparaison Rcpp -", dataset_name),
           x = "Taille des données",
           y = "Temps (s)") +
      theme_minimal() +
      theme(legend.position = "bottom")
  )
}


```


## Simulations qui comparent les temps code R et le code C++


```{r}
one.simu <- function(n, func) {
  # Génère un vecteur aléatoire
  vec <- sample(1:(10 * n), n, replace = TRUE)
  
  # Récupère la fonction à exécuter
  f <- if (is.character(func)) match.fun(func) else func
  
  # Mesure du temps d’exécution
  time <- system.time(f(vec))["elapsed"]
  return(time)
}


# Définir le nombre de simulations et la taille des données
nbSimus <- 10  # Le nombre de simulations
vector_n <- seq(from = 100, to = 50100, length.out = nbSimus)

# Tri Base Rcpp
res_base <- data.frame(matrix(0, nbSimus, 2))
colnames(res_base) <- c("n", "Time")
for (j in 1:nbSimus) {
  n_val <- vector_n[j]
  res_base[j,] <- c(n_val, one.simu(n_val, func = "tri_base_Rcpp"))
}

# Init pour R
res_base_R <- data.frame(matrix(0, nbSimus, 2))
colnames(res_base_R) <- c("n", "Time")

# Boucle pour R
for (j in 1:nbSimus) {
  n_val <- vector_n[j]
  res_base_R[j,] <- c(n_val, one.simu(n_val, func = "tri_base"))
}

# Création du dataframe de comparaison avec les résultats
df_compar <- data.frame(
  n = vector_n,
  base_R = res_base_R$Time,
  base_Rcpp = res_base$Time
)

# Transformation des résultats en format long pour ggplot
library(tidyr)
df_long <- pivot_longer(df_compar, cols = -n, names_to = "algo", values_to = "temps")

# Comparaison pour l'algorithme de tri base
df_base <- data.frame(
  n = vector_n,
  base_R = res_base_R$Time,
  base_Rcpp = res_base$Time
)
df_base_long <- pivot_longer(df_base, cols = -n, names_to = "algo", values_to = "temps")

library(ggplot2)
ggplot(df_base_long, aes(x = n, y = temps, color = algo)) +
  geom_line(size = 1) +
  labs(title = "Comparaison R vs Rcpp - Tri Base",
       x = "Taille des données", y = "Temps (s)") +
  theme_minimal()

# Calcul du ratio des temps R / Rcpp pour l'algorithme tri_base
df_compar_ratio <- data.frame(
  n = vector_n,
  base_R_vs_Rcpp = res_base_R$Time / res_base$Time
)

# Affichage des ratios
print("Comparaison des ratios Time_R / Time_Rcpp pour l'algorithme Tri Base:")
print(df_compar_ratio)


```

## Simulations qui montrent que la complexité attendue est 

```{r}



```















```{r, echo=FALSE}
library(M2algorithmique)
```

# En R
```{r, echo=FALSE}
n <- 10^7
vec <- sample(1:100, n, replace = TRUE)
start_time <- Sys.time()
sorted_fusion <- tri_fusion(vec)
end_time <- Sys.time()
print(as.numeric(difftime(end_time, start_time, units = "secs")))
```

# En C++
```{r}
n <- 10^9
vec <- sample(1:100, n, replace = TRUE)
start_time <- Sys.time()
sorted_fusion <- tri_fusion_Rcpp(vec)
end_time <- Sys.time()
print(as.numeric(difftime(end_time, start_time, units = "secs")))
```




```{r, echo=FALSE}

# Comparaison des performances
set.seed(42)
n <- 10000
sizes <- seq(from = 100, to = n, length.out = 50)

times_fusion <- numeric(length(sizes))
times_radix <- numeric(length(sizes))

for (i in seq_along(sizes)) {
  vec <- sample(1:100, sizes[i], replace = TRUE)
  
  # Temps pour le tri fusion
  start_time <- Sys.time()
  sorted_fusion <- tri_fusion(vec)
  end_time <- Sys.time()
  times_fusion[i] <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  # Temps pour le tri par quick
  start_time <- Sys.time()
  sorted_radix <- tri_quick(vec)
  end_time <- Sys.time()
  times_radix[i] <- as.numeric(difftime(end_time, start_time, units = "secs"))
}

# Tracer un graphique avec l'axe X en échelle logarithmique
plot(sizes, times_fusion, type = "o", log = "x", col = "red", 
     xlab = "Taille du vecteur (log)", ylab = "Temps (s)", 
     main = "Comparaison des performances des algorithmes")


lines(sizes, times_radix, type = "o", col = "blue", pch = 16)
legend("topleft", legend = c("Tri fusion", "Tri par base"), col = c("red", "blue"), pch = 16)

```


```{r, echo=FALSE}
# Ajustement de la régression linéaire pour le tri par base
model_radix <- lm(times_radix ~ sizes)

# Affichage du nuage de points
plot(sizes, times_radix, col = "blue", pch = 16, 
     xlab = "Taille du vecteur", ylab = "Temps (s)", 
     main = "Régression linéaire Tri par Base (O(n))")

# Ajout de la droite de régression
abline(model_radix, col = "black", lwd = 2)


```

```{r}
set.seed(42)
n <- 100  # Taille des listes
k_values <- 1:15  # Valeurs de k

times_fusion <- numeric(length(k_values))
times_radix <- numeric(length(k_values))

for (k in k_values) {
  # Générer une liste d'entiers entre 0 et 10^k - 1
  vec <- sample((10^(k-1) - 1):(10^k - 1), n, replace = TRUE)
  
  # Temps pour le tri fusion O(n log n)
  start_time <- Sys.time()
  sorted_fusion <- tri_fusion(vec)
  end_time <- Sys.time()
  times_fusion[k] <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  # Temps pour le tri par base O(n k)
  start_time <- Sys.time()
  sorted_radix <- tri_base(vec)
  end_time <- Sys.time()
  times_radix[k] <- as.numeric(difftime(end_time, start_time, units = "secs"))
}

# Tracer les performances
plot(k_values, times_fusion, type = "o", col = "red", xlab = "Nombre de chiffres k", 
     ylab = "Temps (s)", main = "Complexité des tris en fonction de k", pch = 16, ylim = range(c(times_fusion, times_radix)))

lines(k_values, times_radix, type = "o", col = "blue", pch = 16)

legend("topleft", legend = c("Tri fusion O(n log n)", "Tri par base O(n k)"), 
       col = c("red", "blue"), pch = 16)


```


```{r}
set.seed(42)
n <- 1000   # Taille des listes
k_values <- 1:15  # Valeurs de k

times_fusion <- numeric(length(k_values))
times_radix <- numeric(length(k_values))

for (k in k_values) {
  # Générer une liste d'entiers entre 0 et 10^k - 1
  vec <- sample((10^(k-1) - 1):(10^k - 1), n, replace = TRUE)
  
  # Temps pour le tri fusion O(n log n)
  start_time <- Sys.time()
  sorted_fusion <- tri_fusion(vec)
  end_time <- Sys.time()
  times_fusion[k] <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  # Temps pour le tri par base O(n k)
  start_time <- Sys.time()
  sorted_radix <- tri_base(vec)
  end_time <- Sys.time()
  times_radix[k] <- as.numeric(difftime(end_time, start_time, units = "secs"))
}

# Tracer les performances
plot(k_values, times_fusion, type = "o", col = "red", xlab = "Nombre de chiffres k", 
     ylab = "Temps (s)", main = "Complexité des tris en fonction de k", pch = 16, ylim = range(c(times_fusion, times_radix)))

lines(k_values, times_radix, type = "o", col = "blue", pch = 16)

legend("topleft", legend = c("Tri fusion O(n log n)", "Tri par base O(n k)"), 
       col = c("red", "blue"), pch = 16)
```

